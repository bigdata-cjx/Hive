# 了解数据压缩
hive 的数据压缩 == MR的数据压缩
## 在哪个阶段进行数据压缩
### MR阶段过程
input -> map -> shuffle -> reduce -> output
### shuffle 阶段几个重要内容：
分区 -> 排序 -> combiner(map端的reduce) -> 压缩 -> 分组
### compression的阶段
map 输出  和   reduce 输出
## 为什么要数据压缩
1. 节省磁盘空间
2. 减小网络IO、磁盘IO
## 有哪些数据压缩方案
1. zlib 
2. gzip
3. bzip2
4. snappy（google 企业应用）
5. lz4
6. lzo

压缩效率：bzip2 > gzip > lzo 
解压速度：lzo > gzip > bzip2
# hadoop编译(添加 snappy 库)
## 检查本地库，查看是否有snappy压缩库
```
root@bigdata:/opt/modules/hadoop-2.6.0# bin/hadoop checknative
21/07/12 09:13:48 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
21/07/12 09:13:48 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
21/07/12 09:13:48 ERROR snappy.SnappyCompressor: failed to load SnappyCompressor
java.lang.UnsatisfiedLinkError: Cannot load libsnappy.so.1 (libsnappy.so.1: cannot open shared object file: No such file or directory)!
	at org.apache.hadoop.io.compress.snappy.SnappyCompressor.initIDs(Native Method)
	at org.apache.hadoop.io.compress.snappy.SnappyCompressor.<clinit>(SnappyCompressor.java:61)
	at org.apache.hadoop.io.compress.SnappyCodec.isNativeCodeLoaded(SnappyCodec.java:79)
	at org.apache.hadoop.util.NativeLibraryChecker.main(NativeLibraryChecker.java:82)
Native library checking:
hadoop:  true /opt/modules/hadoop-2.6.0/lib/native/libhadoop.so.1.0.0
zlib:    true /lib/x86_64-linux-gnu/libz.so.1
snappy:  false 
lz4:     true revision:99
bzip2:   false 
```
## 下载Hadoop源码
https://archive.apache.org/dist/hadoop/common/hadoop-2.6.0/hadoop-2.6.0-src.tar.gz
## 具体编译过程，看其他几个文件
## 将编译结果发送到开发环境
```
➜  hadoop2.6.0编译snappy结果文件 scp -r root@bigdata-pro02:/opt/modules/hadoop-2.6.0-src/hadoop-dist/target/hadoop-2.6.0/lib/native ./
➜  hadoop2.6.0编译snappy结果文件 ll native 
总用量 7.0M
-rw-r--r-- 1 cjx cjx 1.3M 7月  17 16:37 libhadoop.a
-rw-r--r-- 1 cjx cjx 1.6M 7月  17 16:37 libhadooppipes.a
-rwxr-xr-x 1 cjx cjx 751K 7月  17 16:37 libhadoop.so
-rwxr-xr-x 1 cjx cjx 751K 7月  17 16:37 libhadoop.so.1.0.0
-rw-r--r-- 1 cjx cjx 464K 7月  17 16:37 libhadooputils.a
-rw-r--r-- 1 cjx cjx 417K 7月  17 16:37 libhdfs.a
-rwxr-xr-x 1 cjx cjx 263K 7月  17 16:37 libhdfs.so
-rwxr-xr-x 1 cjx cjx 263K 7月  17 16:37 libhdfs.so.0.0.0
-rw-r--r-- 1 cjx cjx 511K 7月  17 16:37 libsnappy.a
-rwxr-xr-x 1 cjx cjx  955 7月  17 16:37 libsnappy.la
-rwxr-xr-x 1 cjx cjx 253K 7月  17 16:37 libsnappy.so
-rwxr-xr-x 1 cjx cjx 253K 7月  17 16:37 libsnappy.so.1
-rwxr-xr-x 1 cjx cjx 253K 7月  17 16:37 libsnappy.so.1.3.0

➜  hadoop2.6.0编译snappy结果文件 docker cp native 482324c29c5d:/opt/modules/hadoop-2.6.0/lib/native/ 
root@bigdata:/opt/modules/hadoop-2.6.0/lib/native# mv native/* ./
root@bigdata:/opt/modules/hadoop-2.6.0/lib/native# ll
total 7084
drwxr-xr-x 1 20000 20000    4096 Jul 17 08:42 ./
drwxr-xr-x 1 20000 20000    4096 Nov 13  2014 ../
-rw-r--r-- 1  1000  1000 1325284 Jul 17 08:37 libhadoop.a
-rwxr-xr-x 1  1000  1000  768352 Jul 17 08:37 libhadoop.so*
-rwxr-xr-x 1  1000  1000  768352 Jul 17 08:37 libhadoop.so.1.0.0*
-rw-r--r-- 1  1000  1000 1607128 Jul 17 08:37 libhadooppipes.a
-rw-r--r-- 1  1000  1000  474962 Jul 17 08:37 libhadooputils.a
-rw-r--r-- 1  1000  1000  426938 Jul 17 08:37 libhdfs.a
-rwxr-xr-x 1  1000  1000  268400 Jul 17 08:37 libhdfs.so*
-rwxr-xr-x 1  1000  1000  268400 Jul 17 08:37 libhdfs.so.0.0.0*
-rw-r--r-- 1  1000  1000  522272 Jul 17 08:37 libsnappy.a
-rwxr-xr-x 1  1000  1000     955 Jul 17 08:37 libsnappy.la*
-rwxr-xr-x 1  1000  1000  258632 Jul 17 08:37 libsnappy.so*
-rwxr-xr-x 1  1000  1000  258632 Jul 17 08:37 libsnappy.so.1*
-rwxr-xr-x 1  1000  1000  258632 Jul 17 08:37 libsnappy.so.1.3.0*
drwxr-xr-x 2  1000  1000    4096 Jul 17 08:42 native/
root@bigdata:/opt/modules/hadoop-2.6.0/lib/native# rm -rf native/
root@bigdata:/opt/modules/hadoop-2.6.0# bin/hadoop checknative
21/07/17 08:44:20 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
21/07/17 08:44:20 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library
Native library checking:
hadoop:  true /opt/modules/hadoop-2.6.0/lib/native/libhadoop.so
zlib:    true /lib/x86_64-linux-gnu/libz.so.1
snappy:  true /opt/modules/hadoop-2.6.0/lib/native/libsnappy.so.1
lz4:     true revision:99
bzip2:   false 
openssl: false Cannot load libcrypto.so (libcrypto.so: cannot open shared object file: No such file or directory)!
```
## 测试压缩
### 启动服务
```
service ssh start
export JAVA_HOME=/opt/modules/jdk1.8.0_271
export PATH=$JAVA_HOME/bin:$PATH
/opt/modules/hadoop-2.6.0/sbin/start-dfs.sh
/opt/modules/hadoop-2.6.0/sbin/start-yarn.sh
```
### MR测试
```
root@bigdata:/opt/modules/hadoop-2.6.0# bin/hdfs dfs -text /user/root/data/wordcount.txt
root@bigdata:/opt/modules/hadoop-2.6.0# bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /user/root/data/wordcount.txt /user/root/data/output
root@bigdata:/opt/modules/hadoop-2.6.0# bin/hdfs dfs -text /user/root/data/output/p*

root@bigdata:/opt/modules/hadoop-2.6.0# bin/yarn jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount -Dmapreduce.output.fileoutputformat.compress=true -Dmapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec /user/root/data/wordcount.txt /user/root/data/outputCompress
root@bigdata:/opt/modules/hadoop-2.6.0# bin/hdfs dfs -text /user/root/data/outputCompress/p*
21/07/17 09:10:32 INFO compress.CodecPool: Got brand-new decompressor [.snappy]
c	1
flink	1
flume	1
hadoop	3
hbase	2
hdfs	2
hive	2
java	2
kafka	1
kotlin	2
mysql	1
python	1
scala	2
spark	1
sqoop	1
storm	1
yarn	2
```